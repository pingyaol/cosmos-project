---
title: "Lab-04c-capstone-prep"
author: "Pingyao Liu"
date: "2024-07-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Capstone Prep

## Introduction

A lot of the mechanics behind data science work is messy, but today we're going to do our best to get started on the right foot and sidestep some of that! This document is going to help you get situated when you start working on your capstone project. There are many approaches to an open-ended problem like a research project, and this is just one of them; as you learn what you like and don't like about R and R markdown, about how to structure your work, feel free to change things. Burn it all down and start again, if you choose!

This is just one example of what good might look like.

## Research Question(s)

**Always** start with this - you need to know where your work is headed to help guide all of your analysis.

In the case of this project, and quite often in research, you're going to be given a dataset and you might not have an explicit objective given to you.

Start with an open mind - recall exploratory data analysis. You've got a totally blank slate, so your first task might be to find out what's interesting about a dataset that begets further investigation.

## Initialization

We're going to start building our toolbox by adding our functions. Whenever we want to use a new tool, we're going to come back up here and add it into this code block.

```{r initialization}
#
# The libraries you're going to use should go here, at the very top.
# Add tidyverse. You know you're gonna use tidyverse!
# Consider a unique random seed: set.seed(ANumberThatYouHappenToLike)
#
library(tidyverse)
library(ggplot2)
library(gtsummary)
library(dplyr)
```

## Data Import

Typically, I split this section into two pieces; one explicitly for imports and merging,     and another explicitly for data cleaning operations.

**An important note** is filtering and trimming your dataset down is different than cleaning; this first section is about getting your data into a 'minimum usable state', renaming things, removing records that are erroneous or full of 'NA' or 'NULL' values, and so forth. I, personally, wouldn't start filtering, subetting, or other such tasks until I was writing code that dealt with those subsets.

**Another important note** is that you should take notes in comments (lines beginning with #) to keep track of what occurs here - it's typically requested when you author research papers to note how many observations were removed, and for what reasons.

```{r dataImport}
#
#read_csv()...
#data <- left_join(data, otherdata, by = matchingFeature)
#
data <- read.csv("../data/parkinsons_data.data")
glimpse(data) %>% head()
``` 


```{r dataCleaning}
#
#drop.na()...
#
data <- rename(data, subject = subject.)
sub_counts <- group_by(data, subject) %>% count(subject)
sub_counts
```
Note: graph of # of recordings, slope of the data (days vs updrs score) 
The dataset has 42 subjects, with 28 males and 14 females. Each subject had a different number of recordings, with the number ranging from 101 to 168. Due to the discrepancy between the number of recordings available to each subject, we decided to <INSERT PLAN HERE>. 

We took the mean of all the metrics. 
```{r}
mean_vals <- data %>% group_by(subject) %>% summarise(across(1: PPE, .fns = mean))
mean_vals
```


## Exploratory Data Analysis

```{r}
gender_count <- group_by(data, sex, subject) %>% count(sex)
gender_count
```


Here's where the majority of your coding effort will likely take place. In doing EDA, you're going to be slicing, filtering, subsetting, and transforming your data. Break this into subsections with specific goals, and make sure to *save* your figures to the folder with appropriate names!

It's hard for me to prescribe what exactly to do here, because where you go varies dataset by dataset, and objective by objective.

Consider some things, if you don't have an explicit goal for your project yet. Are there extant differences between subpopulations in your sample? Are there specific features you want to examine for trends?

Include appropriate comments and notes as you go. This is largely a scratchpad, where things don't necessarily have to be presentable or pretty (yet). This is just a space for you and your team to throw ideas together - check parameters, test things out, and prepare your data for modeling or more 'final' tests.

Make sure to document the driving idea behind each thread of thought; including a sentence or two in a comment to track what you were doing can make it easy for others to follow up on your work, and make it easier for you to come back to an old idea in case you're jumping between tasks (this will happen - frequently! get in the habit of leaving breadcrumbs!)

## Models and Statistical Tests

Once your data is in a workable state, here is where you'll run regressions and comparative tests to generate results to test the statistical questions you landed on after performing exploratory data analysis.

Operate with the same diligence as before - separate chunks for separate tasks. Annotate everything. 

## Results

Your results can typically be summarized with a set of **four** values, recall:
An estimated value - what it says on the tin!
A p-value is a result's statistical significance -  (lower is better - the chance that your result would have occurred under pure random chance)
A confidence interval - (a range in which we're some percentage [typically 95 or 99%] sure the value can take)
An r-squared value - (how much of the data's total variance is explained by the model - you'll see this early next week!)

Save this section for extracting relevant results from the models and statistical test objects you generate above. Transform, present, and display them as necessary.

## Conclusions

This final part is almost all written; do your findings validate your research question? Are they significant? Are they significant *and* impactful? Are they significant, impactful, *and* precise? Two out of three?

Think about and talk about the implications of what you've discovered.